# 工作流程
爬行和抓取
- 蜘蛛：robots.txt
- 跟踪链接：广度优先/深度优先
- 吸引蜘蛛：网站和页面权重；页面更新频率；导入链接(```<a>```标签？)；与首页点击距离；URL结构（短的、层次浅的URL可能被认为权重较高）
- 地址库：蜘蛛发现链接后并不是马上访问，而是将URL存入地址库，然后统一安排抓取
- 复制内容检测：有些蜘蛛在爬取内容时会进行检测，如果发现是低权重的网站上复制、转载的内容，往往会停止爬取；

预处理

也被称为索引。抓取过来的页面必须经过预处理，为最后的查询排名做好准备。
- 提取文字：提取html标签内的文字，meta中的文字，图片替代文字，链接锚文字等；
- 中文分词：分为基于词典匹配，基于统计两种方法；百度更倾向于搜索词的完整查找，google倾向于细碎划分；
- 去掉止词：搜索引擎会在索引页面之前去掉“的，地，得，啊，哈，呀，以，却，从而”等助词、感叹词、副词、介词等，以减少无谓的计算量。这些词被称为停止词。英文中有the,a,an,to,of等。
- 消除噪声：页头、导航、页脚、广告等区域的内容往往属于噪声，有很多重复出现的无意义的关键词（如果，分类、历史、档案等），需要消噪。
- 去重：同一篇文章经常会出现在不同网站上，搜索引擎会对其做页面特征关键词指纹计算，比如采用MD5算法。因此即使对文章简单地增加“的”，“地”，“得”，或者调换文章段落顺序等方式，并不会欺骗去重算法，无法伪装成原创内容。