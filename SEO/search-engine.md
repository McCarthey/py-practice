# 工作流程
爬行和抓取
- 蜘蛛：robots.txt
- 跟踪链接：广度优先/深度优先
- 吸引蜘蛛：网站和页面权重；页面更新频率；导入链接(```<a>```标签？)；与首页点击距离；URL结构（短的、层次浅的URL可能被认为权重较高）
- 地址库：蜘蛛发现链接后并不是马上访问，而是将URL存入地址库，然后统一安排抓取
- 复制内容检测：有些蜘蛛在爬取内容时会进行检测，如果发现是低权重的网站上复制、转载的内容，往往会停止爬取；

预处理

也被称为索引。抓取过来的页面必须经过预处理，为最后的查询排名做好准备。
- 提取文字：提取html标签内的文字，meta中的文字，图片替代文字，链接锚文字等；
- 中文分词：分为基于词典匹配，基于统计两种方法；百度更倾向于搜索词的完整查找，google倾向于细碎划分；
- 去掉止词：搜索引擎会在索引页面之前去掉“的，地，得，啊，哈，呀，以，却，从而”等助词、感叹词、副词、介词等，以减少无谓的计算量。这些词被称为停止词。英文中有the,a,an,to,of等。
- 消除噪声：页头、导航、页脚、广告等区域的内容往往属于噪声，有很多重复出现的无意义的关键词（如果，分类、历史、档案等），需要消噪。
- 去重：同一篇文章经常会出现在不同网站上，搜索引擎会对其做页面特征关键词指纹计算，比如采用MD5算法。因此即使对文章简单地增加“的”，“地”，“得”，或者调换文章段落顺序等方式，并不会欺骗去重算法，无法伪装成原创内容。
- 正向索引（索引）：经过文字提取、分词、消噪、去重后，搜索引擎的得到的就是以词为单位的字符串。接下来提取关键词，把页面文件转换为一个关键词组成的集合，即页面————关键词集合的映射关系。
- 倒排索引：搜索引擎会将正向索引数据重新构造成倒排索引，转换成关键词————页面的映射。这样当用户搜索某个关键词时，排序程序就可以马上找出包含这个关键词的文件。
- 链接关系计算：是预处理中重要的部分。这些链接指向关系形成了网站和页面的链接权重。
- 特殊文件处理：处理其他以文字为基础的文件类型，PDF，Word，TXT等。
- 质量判断：搜索引擎会根据关键词、链接、用户体验、排版、广告布局、语法、页面打开速度等各种因素预先计算页面质量。

排名

- 搜索词处理：中文分词、去停止词、指令处理、拼写错误矫正、整合搜索触发、搜索框提示。
- 文件匹配
- 初始子集的选择：通常搜索引擎只需计算几百至一千个搜素结果的相关性
- **相关性计算**：

    - 关键词常用程度
    - 词频及密度
    - 关键词位置及形式
    - 关键词距离
    - 链接分析及页面权重
- 排名过滤及调整：主要对于有作弊嫌疑的页面施加惩罚
- 排名显示：将排名结果显示在页面上。有时候需要动态生成页面摘要，而不是调用页面本身的说明标签
- 搜索缓存：缓存用户的查询结果，避免资源浪费
- 日志：搜索用户的IP地址、查询词、搜索时间、点击了哪些结果等都被记录形成日志

